{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1739a21-cd50-4ffa-bcb8-3562487866ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1983584/1753801065.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# coding: utf-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/torch17/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    " \n",
    "from keras.models import Model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from pylab import *\n",
    "import keras\n",
    " \n",
    " \n",
    "def get_row_col(num_pic):\n",
    "    squr = num_pic ** 0.5\n",
    "    row = round(squr)\n",
    "    col = row + 1 if squr - row > 0 else row\n",
    "    return row, col\n",
    " \n",
    " \n",
    "def visualize_feature_map(img_batch):\n",
    "    feature_map = np.squeeze(img_batch, axis=0)\n",
    "    print(feature_map.shape)\n",
    " \n",
    "    feature_map_combination = []\n",
    "    plt.figure()\n",
    " \n",
    "    num_pic = feature_map.shape[2]\n",
    "    row, col = get_row_col(num_pic)\n",
    " \n",
    "    for i in range(0, num_pic):\n",
    "        feature_map_split = feature_map[:, :, i]\n",
    "        feature_map_combination.append(feature_map_split)\n",
    "        plt.subplot(row, col, i + 1)\n",
    "        plt.imshow(feature_map_split)\n",
    "        axis('off')\n",
    "        title('feature_map_{}'.format(i))\n",
    " \n",
    "    plt.savefig('feature_map.png')\n",
    "    plt.show()\n",
    " \n",
    "    # 各个特征图按1：1 叠加\n",
    "    feature_map_sum = sum(ele for ele in feature_map_combination)\n",
    "    plt.imshow(feature_map_sum)\n",
    "    plt.savefig(\"feature_map_sum.png\")\n",
    " \n",
    " \n",
    "def create_model():\n",
    "    model = Sequential()\n",
    " \n",
    "    # 第一层CNN\n",
    "    # 第一个参数是卷积核的数量，第二三个参数是卷积核的大小\n",
    "    model.add(Convolution2D(9, 5, 5, input_shape=img.shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    " \n",
    "    # 第二层CNN\n",
    "    model.add(Convolution2D(9, 5, 5, input_shape=img.shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    " \n",
    "    # 第三层CNN\n",
    "    model.add(Convolution2D(9, 5, 5, input_shape=img.shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "    # 第四层CNN\n",
    "    model.add(Convolution2D(9, 3, 3, input_shape=img.shape))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "    return model\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    img = cv2.imread('001.jpg')\n",
    " \n",
    "    model = create_model()\n",
    " \n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    conv_img = model.predict(img_batch)  # conv_img 卷积结果\n",
    " \n",
    "    visualize_feature_map(conv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888525ef-c7c8-4640-adbc-ed4884e045b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /home/liaoruizhi/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e40caadc604c21b26e57001cf8f194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /tmp/pip-req-build-dwj9trv5/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1983584/692667966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /tmp/pip-req-build-dwj9trv5/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x\n",
    "\n",
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output  = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output)\n",
    "        return target_activations, output\n",
    "\n",
    "def preprocess_image(img):\n",
    "    means=[0.485, 0.456, 0.406]\n",
    "    stds=[0.229, 0.224, 0.225]\n",
    "\n",
    "    preprocessed_img = img.copy()[: , :, ::-1]\n",
    "    for i in range(3):\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
    "    preprocessed_img = \\\n",
    "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
    "    preprocessed_img = torch.from_numpy(preprocessed_img)\n",
    "    preprocessed_img.unsqueeze_(0)\n",
    "    input = Variable(preprocessed_img, requires_grad = True)\n",
    "    return input\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite(\"../../images/cam01.jpg\", np.uint8(255 * cam))\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input) \n",
    "\n",
    "    def __call__(self, input, index = None):\n",
    "        if self.cuda:\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        #one_hot.backward(retain_variables=True)\n",
    "        one_hot.backward()\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        weights = np.mean(grads_val, axis = (2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1 : ], dtype = np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "class GuidedBackpropReLU(Function):\n",
    "\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
    "        self.save_for_backward(input, output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, output = self.saved_tensors\n",
    "        grad_input = None\n",
    "\n",
    "        positive_mask_1 = (input > 0).type_as(grad_output)\n",
    "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "class GuidedBackpropReLUModel:\n",
    "    def __init__(self, model, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        # replace ReLU with GuidedBackpropReLU\n",
    "        for idx, module in self.model.features._modules.items():\n",
    "            if module.__class__.__name__ == 'ReLU':\n",
    "                self.model.features._modules[idx] = GuidedBackpropReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index = None):\n",
    "        if self.cuda:\n",
    "            output = self.forward(input.cuda())\n",
    "        else:\n",
    "            output = self.forward(input)\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        # self.model.features.zero_grad()\n",
    "        # self.model.classifier.zero_grad()\n",
    "        one_hot.backward()\n",
    "\n",
    "        output = input.grad.cpu().data.numpy()\n",
    "        output = output[0,:,:,:]\n",
    "\n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python grad_cam.py <path_to_image>\n",
    "    1. Loads an image with opencv.\n",
    "    2. Preprocesses it for VGG19 and converts to a pytorch variable.\n",
    "    3. Makes a forward pass to find the category index with the highest score,\n",
    "    and computes intermediate activations.\n",
    "    Makes the visualization. \"\"\"\n",
    "\n",
    "    image_path = \"../../images/dog-cat.jpg\"\n",
    "\n",
    "    # Can work with any model, but it assumes that the model has a \n",
    "    # feature method, and a classifier method,\n",
    "    # as in the VGG models in torchvision.\n",
    "    grad_cam = GradCam(model = models.vgg19(pretrained=True), \\\n",
    "                    target_layer_names = [\"35\"], use_cuda=True)\n",
    "\n",
    "    img = cv2.imread(image_path, 1)\n",
    "    img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "    input = preprocess_image(img)\n",
    "\n",
    "    # If None, returns the map for the highest scoring category.\n",
    "    # Otherwise, targets the requested index.\n",
    "    target_index = None\n",
    "\n",
    "    mask = grad_cam(input, target_index)\n",
    "\n",
    "    show_cam_on_image(img, mask)\n",
    "\n",
    "    gb_model = GuidedBackpropReLUModel(model = models.vgg19(pretrained=True), use_cuda=True)\n",
    "    gb = gb_model(input, index=target_index)\n",
    "    utils.save_image(torch.from_numpy(gb), '../../images/gb.jpg')\n",
    "\n",
    "    cam_mask = np.zeros(gb.shape)\n",
    "    for i in range(0, gb.shape[0]):\n",
    "        cam_mask[i, :, :] = mask\n",
    "\n",
    "    cam_gb = np.multiply(cam_mask, gb)\n",
    "    utils.save_image(torch.from_numpy(cam_gb), '../../images/cam_gb.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc5c1c-9dd6-42d5-84e9-073b8c829b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradcam(model, data, label, class_dim=16):\n",
    "    data, label = paddle.to_tensor(data), paddle.to_tensor(label)\n",
    "\n",
    "    # se_resnet50vd的forward过程\n",
    "    y = model.conv1_1(data)\n",
    "    y = model.conv1_2(y)\n",
    "    y = model.conv1_3(y)\n",
    "    y = model.pool2d_max(y)\n",
    "    for block in model.block_list:\n",
    "        y = block(y)\n",
    "    conv = y # 得到模型最后一个卷积层的特征图\n",
    "    y = model.pool2d_avg(y)\n",
    "    y = paddle.reshape(y, shape=[-1, model.pool2d_avg_channels])\n",
    "    predict = model.out(y) # 得到前向计算的结果\n",
    "    \n",
    "    label = paddle.reshape(label, [-1])\n",
    "    predict_one_hot = paddle.nn.functional.one_hot(label, class_dim) * predict # 将模型输出转化为one-hot向量\n",
    "    score = paddle.mean(predict_one_hot) # 得到预测结果中概率最高的那个分类的值\n",
    "    score.backward() # 反向传播计算梯度\n",
    "    grad_map = conv.grad # 得到目标类别的loss对最后一个卷积层输出的特征图的梯度\n",
    "    grad = paddle.mean(paddle.to_tensor(grad_map), (2, 3), keepdim=True) # 对特征图的梯度进行GAP（全局平局池化）\n",
    "    gradcam = paddle.sum(grad * conv, axis=1) # 将最后一个卷积层输出的特征图乘上从梯度求得权重进行各个通道的加和\n",
    "    gradcam = paddle.maximum(gradcam, paddle.to_tensor(0.)) # 进行ReLU操作，小于0的值设为0\n",
    "    for j in range(gradcam.shape[0]):\n",
    "        gradcam[j] = gradcam[j] / paddle.max(gradcam[j]) # 分别归一化至[0, 1]\n",
    "    return gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c217e202-be13-46e9-9a2e-a2daaeedcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.randn([1,32,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdad6f44-ec62-4b74-ace9-be48591008c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a.reshape(1,800,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617ef2ec-30d4-42d3-a8d8-4f80248018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.zeros(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a93bde5-4bf8-4515-980a-196d06789aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5854a516-1d93-48a6-aeac-685c2fc7a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "        \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_attention_heads, input_size, hidden_size, hidden_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = hidden_size\n",
    "\n",
    "        self.query = nn.Linear(input_size, self.all_head_size)\n",
    "        self.key = nn.Linear(input_size, self.all_head_size)\n",
    "        self.value = nn.Linear(input_size, self.all_head_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        # 做完self-attention 做一个前馈全连接 LayerNorm 输出\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        # [batch_size heads seq_len seq_len] scores\n",
    "        # [batch_size 1 1 seq_len]\n",
    "\n",
    "        # attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        # Fixme\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80ec8bb3-3636-4c53-8a56-32f9186dae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=SelfAttention(2,84,84,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb5ab04-f157-4151-879d-222b52e5a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (key): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (value): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (attn_dropout): Dropout(p=0, inplace=False)\n",
       "  (dense): Linear(in_features=84, out_features=84, bias=True)\n",
       "  (LayerNorm): LayerNorm()\n",
       "  (out_dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b0e2bfa-1782-4335-a1a8-2e6586dda712",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(3,84,84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5afb7252-d606-4dea-8075-3bb2d6942204",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=sf(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfca2dd1-da45-4232-8c85-28a0aebe7961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 84, 84])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939c4bb-93bc-43b5-a44a-262e1f5da500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch17]",
   "language": "python",
   "name": "conda-env-torch17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
